{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11631688,"sourceType":"datasetVersion","datasetId":6775341}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"255aa798-0695-4021-a1e4-6fe7c01eb6f5","_cell_guid":"11caef8d-61e3-484f-9c9d-1d3023f2b898","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc, log_loss\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier","metadata":{"_uuid":"9476aed7-66c4-4aef-b7d9-e14f33cf17d2","_cell_guid":"3da65de5-b984-4230-882f-14d988b14739","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-12T13:17:30.960528Z","iopub.execute_input":"2025-05-12T13:17:30.960786Z","iopub.status.idle":"2025-05-12T13:17:36.252749Z","shell.execute_reply.started":"2025-05-12T13:17:30.960760Z","shell.execute_reply":"2025-05-12T13:17:36.251209Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#show keys of the first subject to explore dataset shape\ndata_path = \"/kaggle/input/wesad-full-dataset/WESAD\"\nwesad_files = []\nfor root, dirs, files in os.walk(data_path):\n    for file in files:\n        if file.endswith(\".pkl\"):\n            wesad_files.append(os.path.join(root, file))\nif wesad_files:\n    sample_file = wesad_files[0]\n    with open(sample_file, 'rb') as f:\n        data = pickle.load(f, encoding='latin1')\n    print(\"Keys in the dataset:\", data.keys())\nelse:\n    print(\"No .pkl files found in the directory.\")","metadata":{"_uuid":"e53e44f5-8f26-4032-9918-45edfb52b9ca","_cell_guid":"145951a3-6a44-42eb-a064-1552091ba172","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-12T13:17:40.501052Z","iopub.execute_input":"2025-05-12T13:17:40.501463Z","iopub.status.idle":"2025-05-12T13:17:50.864657Z","shell.execute_reply.started":"2025-05-12T13:17:40.501430Z","shell.execute_reply":"2025-05-12T13:17:50.863298Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#show more about shape\nprint(\"Signal Data Keys:\", data['signal'].keys())\nfor key in data['signal']:\n    print(f\"{key}: {np.array(data['signal'][key]).shape}\") \nprint(\"Unique Labels:\", np.unique(data['label']))","metadata":{"_uuid":"ee8808ee-f6ee-4be5-9562-69cd0cc5653b","_cell_guid":"735dbb92-b3ed-4019-9f25-dc7eb8985fd1","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-12T13:47:05.254731Z","iopub.execute_input":"2025-05-12T13:47:05.255231Z","iopub.status.idle":"2025-05-12T13:47:05.337622Z","shell.execute_reply.started":"2025-05-12T13:47:05.255190Z","shell.execute_reply":"2025-05-12T13:47:05.336537Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#which from chest and which from wrist\nprint(\"Chest Data Keys:\", data['signal']['chest'].keys())\nprint(\"Wrist Data Keys:\", data['signal']['wrist'].keys())\n# show shape of data inside \nfor key in data['signal']['chest']:\n    print(f\"Chest - {key}: {np.array(data['signal']['chest'][key]).shape}\")\n\nfor key in data['signal']['wrist']:\n    print(f\"Wrist - {key}: {np.array(data['signal']['wrist'][key]).shape}\")\n#we will be dealing with ECG signal","metadata":{"_uuid":"b3ef764b-ae72-437e-a2fc-008beea6a15d","_cell_guid":"7d732c7f-44e6-4c32-8a96-022cdc1fc55d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-12T13:47:09.778848Z","iopub.execute_input":"2025-05-12T13:47:09.779358Z","iopub.status.idle":"2025-05-12T13:47:10.017629Z","shell.execute_reply.started":"2025-05-12T13:47:09.779306Z","shell.execute_reply":"2025-05-12T13:47:10.016532Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#show shape of 3000 points of the ECG signal\nplt.figure(figsize=(15, 5))\nplt.plot(data['signal']['chest']['ECG'][:3000])\nplt.title(\"Chest ECG Signal\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Amplitude\")","metadata":{"_uuid":"cc20edc3-20c1-414c-a223-14bac0d2c346","_cell_guid":"886fca42-d628-48d0-988e-c6b2c2365900","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-12T13:47:12.228743Z","iopub.execute_input":"2025-05-12T13:47:12.229165Z","iopub.status.idle":"2025-05-12T13:47:12.544270Z","shell.execute_reply.started":"2025-05-12T13:47:12.229130Z","shell.execute_reply":"2025-05-12T13:47:12.542831Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#functions to remove outliers and normalize the signal (MIN-MAX)\ndef remove_outliers(data):\n    Q1 = np.percentile(data, 25, axis=0)\n    Q3 = np.percentile(data, 75, axis=0)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    mask = (data >= lower_bound) & (data <= upper_bound)\n    data_clean = np.where(mask, data, np.mean(data, axis=0))\n    return data_clean\ndef normalize_signal(signal):\n    return (signal - np.min(signal)) / (np.max(signal) - np.min(signal))\ndata_path = \"/kaggle/input/wesad-full-dataset/WESAD\"\n#collect all files\nwesad_files = []\nfor root, dirs, files in os.walk(data_path):\n    for file in files:\n        if file.endswith(\".pkl\"):\n            wesad_files.append(os.path.join(root, file))\n\necg_all = []\nlabels_all = []\nvalid_labels = [1, 2, 3, 4]  # since 0 undefined , 5/6/7 will not be considered\n\n# walk through subject to collect pkl files and process them\nfor file in wesad_files:\n    with open(file, 'rb') as f:\n        data = pickle.load(f, encoding='latin1')\n    if 'chest' not in data['signal'] or 'ECG' not in data['signal']['chest']:\n       continue \n    labels = data['label']\n    ecg = data['signal']['chest']['ECG']\n    # filter unimportant labels and ecg signals\n    valid_indices = np.isin(labels, valid_labels)\n    filtered_labels = labels[valid_indices]\n    filtered_ecg = ecg[valid_indices]\n    # Signal processing pipeline\n    filtered_ecg = remove_outliers(filtered_ecg)\n    filtered_ecg = normalize_signal(filtered_ecg)\n    # binary conversion of labels\n    binary_labels = np.where(filtered_labels == 2, 1, 0)\n\n    # add at one loop and repeat \n    ecg_all.append(filtered_ecg)\n    labels_all.append(binary_labels)\n\n#  after loop collect all\necg_all = np.concatenate(ecg_all, axis=0)\nlabels_all = np.concatenate(labels_all, axis=0)\nprint(\"Final ECG shape:\", ecg_all.shape)\nprint(\"Final Labels shape:\", labels_all.shape)","metadata":{"_uuid":"d828a3d8-e029-4952-a52d-0c2b513a8e4f","_cell_guid":"39a341ea-f846-4c79-9269-d6c667371ce8","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-12T13:17:59.583891Z","iopub.execute_input":"2025-05-12T13:17:59.584516Z","iopub.status.idle":"2025-05-12T13:20:44.637953Z","shell.execute_reply.started":"2025-05-12T13:17:59.584478Z","shell.execute_reply":"2025-05-12T13:20:44.636207Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#comare between original labels and binary distributions\nall_original_labels = []\nprint(\"Collecting original labels from all subjects...\")\nfor file in tqdm(wesad_files, desc=\"Processing Files (Original Labels)\"):\n    with open(file, 'rb') as f:\n        data = pickle.load(f, encoding='latin1')\n    \n    all_original_labels.append(data['label'])\n\nall_original_labels = np.concatenate(all_original_labels)\nunique_labels_orig, counts_orig = np.unique(all_original_labels, return_counts=True)\n\nunique_labels_bin, counts_bin = np.unique(labels_all, return_counts=True)\n\n# draw\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# original labels distribution as first graph\nsns.barplot(x=unique_labels_orig, y=counts_orig, palette=\"coolwarm\", ax=axes[0])\naxes[0].set_title(\"Original Label Distribution (0 to 7)\")\naxes[0].set_xlabel(\"Original Labels\")\naxes[0].set_ylabel(\"Count\")\n\n# binary labels distribution as second graph\nsns.barplot(x=unique_labels_bin, y=counts_bin, palette=\"coolwarm\", ax=axes[1])\naxes[1].set_title(\"Binary Label Distribution (0 = No Stress, 1 = Stress)\")\naxes[1].set_xlabel(\"Binary Labels\")\naxes[1].set_ylabel(\"Count\")\naxes[1].set_xticks([0, 1])\naxes[1].set_xticklabels([\"No Stress\", \"Stress\"])\n\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"daba7e82-1623-45ee-abbf-68c84ad62e47","_cell_guid":"b888c2f1-208b-49d4-aad6-45e404543498","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-12T13:47:18.562621Z","iopub.execute_input":"2025-05-12T13:47:18.563029Z","iopub.status.idle":"2025-05-12T13:50:00.222213Z","shell.execute_reply.started":"2025-05-12T13:47:18.562992Z","shell.execute_reply":"2025-05-12T13:50:00.220939Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ecg_data = ecg_all\nbinary_labels = labels_all\n# confirm\nprint(\"Binary Labels (1 = Stressed, 0 = Not Stressed):\")\nprint(binary_labels)\nprint(\"Filtered ECG Shape:\", ecg_data.shape)\nprint(\"Filtered Labels Shape:\", binary_labels.shape)\n#after\nunique_labels, counts = np.unique(binary_labels, return_counts=True)\nprint(\"Count of each class after filtering:\", dict(zip(unique_labels, counts)))","metadata":{"_uuid":"4f3159bc-c8e4-49d0-b0f1-fe3e28e0b545","_cell_guid":"a574e6e2-4c9a-413a-95c3-ea3281f401b4","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-12T13:20:53.099850Z","iopub.execute_input":"2025-05-12T13:20:53.100310Z","iopub.status.idle":"2025-05-12T13:20:53.876859Z","shell.execute_reply.started":"2025-05-12T13:20:53.100267Z","shell.execute_reply":"2025-05-12T13:20:53.875232Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"window_size = 7000  # 700 (samiling rate) x 10 (s)\nstride = window_size  # no overlapping \n# Divide ECG data into windows without overlap using last timestamp method\n#def create_windows(ecg_data, labels, window_size, stride):\n  #  windows = []\n  #  window_labels = []\n   # for i in range(0, len(ecg_data) - window_size + 1, stride):\n   #     windows.append(ecg_data[i:i+window_size])\n   #     window_labels.append(labels[i + window_size - 1])\n  #  return np.array(windows), np.array(window_labels)\n\n# Divide ECG data into windows without overlap using majority voting method\ndef create_windows_majority(ecg_data, labels, window_size, stride):\n    windows = []\n    window_labels = []\n\n    for i in range(0, len(ecg_data) - window_size + 1, stride):\n        window = ecg_data[i:i+window_size]\n        label_window = labels[i:i+window_size]\n\n        majority_label = int(np.round(np.mean(label_window))) \n\n        windows.append(window)\n        window_labels.append(majority_label)\n\n    return np.array(windows), np.array(window_labels)\necg_windows, window_labels = create_windows_majority(ecg_data, binary_labels, window_size, stride)   \n#ecg_windows, window_labels = create_windows(ecg_data, binary_labels, window_size, stride)\n# visualize first 3 windows\nplt.figure(figsize=(15, 5))\nfor i in range(3):\n    plt.subplot(3, 1, i+1)\n    plt.plot(ecg_windows[i])\n    #plt.plot(emg_windows[i])\n    #plt.plot(eda_windows[i])\n    plt.title(f\"ECG Window {i+1}\")\n    #plt.title(f\"EMG Window {i+1}\")\n    #plt.title(f\"EDA Window {i+1}\")\nplt.show()\n\nprint(\"Shape of windows:\", ecg_windows.shape)\nprint(\"Shape of window labels:\", window_labels.shape)","metadata":{"_uuid":"b3be92a0-0481-4314-8f8a-3c3c477d1742","_cell_guid":"e6383df2-2999-4443-8331-490873801393","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-12T13:39:31.308793Z","iopub.execute_input":"2025-05-12T13:39:31.309333Z","iopub.status.idle":"2025-05-12T13:39:32.177636Z","shell.execute_reply.started":"2025-05-12T13:39:31.309292Z","shell.execute_reply":"2025-05-12T13:39:32.176009Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to extract features from each window\ndef extract_features(windows):\n    features = []\n    for window in windows:\n        # Extracting statistical features\n        mean = np.mean(window)\n        std = np.std(window)\n        max_val = np.max(window)\n        min_val = np.min(window)\n        # Adding features to the list\n        features.append([mean, std, max_val, min_val])\n    \n    return np.array(features)\n\n# Extract features from the windows\nfeatures = extract_features(ecg_windows)\n\n# Standardize the features\nscaler = StandardScaler()\nfeatures_scaled = scaler.fit_transform(features)\n\n# Visualizing some of the features\nplt.figure(figsize=(10, 5))\nplt.plot(features_scaled[:50, 0], label=\"Mean\")\nplt.plot(features_scaled[:50, 1], label=\"Std\")\nplt.title(\"Feature Extraction Visualization (Mean and Std) for first 50 windows\")\nplt.xlabel(\"Window Index\")\nplt.ylabel(\"Feature Value\")\nplt.legend()\nplt.show()\n\nprint(\"Shape of the feature matrix:\", features_scaled.shape)\n#وريني عدد الفيتشرز ف اول ويندو\nnum_features = features_scaled.shape[1]\nprint(\"num of features taken in first window:\", num_features)\ntotal_features = features_scaled.shape[0] * features_scaled.shape[1]\nprint(\"total num of features in all:\", total_features)","metadata":{"_uuid":"a97b3503-7e5b-4fd0-8197-c92a33acda56","_cell_guid":"c18babcb-4359-4edc-b61f-1325f1040a4d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-12T13:39:39.174670Z","iopub.execute_input":"2025-05-12T13:39:39.175085Z","iopub.status.idle":"2025-05-12T13:39:39.827501Z","shell.execute_reply.started":"2025-05-12T13:39:39.175038Z","shell.execute_reply":"2025-05-12T13:39:39.826285Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# split features_scaled and window_labels\nX_train, X_test, y_train, y_test = train_test_split(\n    features_scaled,\n    window_labels,\n    test_size=0.2,\n    random_state=42,\n    shuffle=True\n)\n# balance training data only\nsmote = SMOTE(random_state=42)\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n\n# 3. visualize new training distribution\nunique_labels_resampled, counts_resampled = np.unique(y_train_resampled, return_counts=True)\nplt.figure(figsize=(8, 5))\nsns.barplot(x=unique_labels_resampled, y=counts_resampled, palette=\"coolwarm\")\nplt.title(\"Balanced Training Data (SMOTE)\")\nplt.xlabel(\"Label (0 = No Stress, 1 = Stress)\")\nplt.ylabel(\"Count\")\nplt.xticks(ticks=[0, 1], labels=[\"No Stress\", \"Stress\"])\nplt.show()\n# models\nmodels = {\n    \"RandomForest\": RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=5, min_samples_leaf=2, random_state=42),\n    \"KNN\": KNeighborsClassifier(n_neighbors=50),\n    \"XGBoost\": XGBClassifier(n_estimators=100, learning_rate=0.05, max_depth=5, use_label_encoder=False, eval_metric='logloss', random_state=42)\n}\n\nresults = {}\n\n#model tain/test\nfor name, model in models.items():\n    print(f\"\\n===== {name} =====\")\n    \n    model.fit(X_train_resampled, y_train_resampled)\n    \n    y_pred = model.predict(X_test)\n    y_score = model.predict_proba(X_test)[:, 1]\n    \n    # evaluations\n    acc_train = model.score(X_train_resampled, y_train_resampled)\n    acc_test = accuracy_score(y_test, y_pred)\n    auc_score = auc(*roc_curve(y_test, y_score)[:2])\n    train_loss = log_loss(y_train_resampled, model.predict_proba(X_train_resampled))\n    test_loss = log_loss(y_test, model.predict_proba(X_test))\n    \n    print(\"Classification Report:\")\n    print(classification_report(y_test, y_pred))\n    print(f\"Training Accuracy: {acc_train:.2f}\")\n    print(f\"Test Accuracy: {acc_test:.2f}\")\n    print(f\"AUC: {auc_score:.2f}\")\n    print(f\"Log Loss (Train): {train_loss:.4f}\")\n    print(f\"Log Loss (Test): {test_loss:.4f}\")\n    \n    results[name] = {\n        \"train_acc\": acc_train,\n        \"test_acc\": acc_test,\n        \"auc\": auc_score,\n        \"log_loss_train\": train_loss,\n        \"log_loss_test\": test_loss,\n        \"conf_matrix\": confusion_matrix(y_test, y_pred)\n    }\n    \n    #ROC Curve\n    fpr, tpr, _ = roc_curve(y_test, y_score)\n    plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {auc_score:.2f})')\n\nplt.plot([0, 1], [0, 1], color='gray', linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve Comparison')\nplt.legend(loc='lower right')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n#Confusion Matrices\nfor name in models:\n    plt.figure(figsize=(6, 4))\n    sns.heatmap(results[name][\"conf_matrix\"], annot=True, fmt='d', cmap=\"Blues\",\n                xticklabels=[\"No Stress\", \"Stress\"], yticklabels=[\"No Stress\", \"Stress\"])\n    plt.title(f\"{name} Confusion Matrix\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.tight_layout()\n    plt.show()","metadata":{"_uuid":"13213eae-e4c7-4ae6-8be3-2618335798f4","_cell_guid":"a414abce-d336-4e5b-a463-a95ac631682f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-12T13:52:11.328491Z","iopub.execute_input":"2025-05-12T13:52:11.328882Z","iopub.status.idle":"2025-05-12T13:52:14.697223Z","shell.execute_reply.started":"2025-05-12T13:52:11.328835Z","shell.execute_reply":"2025-05-12T13:52:14.696029Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}